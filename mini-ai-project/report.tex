\documentclass[11pt, a4paper]{article}  % Set default font and page size


% Load packages and configure them via options
\usepackage[utf8]{inputenc}
\usepackage[left=22mm, right=22mm, top=20mm, bottom=20mm, headheight=40pt, headsep=10pt]{geometry}  % Set margin of page
\usepackage[parfill]{parskip}  % Set spacing between paragraphs
\usepackage{amsmath}  % For the equation environment
\usepackage{graphicx}  % Required for inserting images
\usepackage[dvipsnames]{xcolor}  % Access color codes
\usepackage[colorlinks=true]{hyperref}  % Enable hyperlink
\usepackage{cleveref}  % Enable easy references to sections, figures, and tables
\usepackage{minted}  % Enable code formatting in LaTeX
\usepackage{changepage}  % Required for \adjustwidth
\usepackage{caption}
\usepackage{enumitem}  % Requried for changing configurations for list
\usepackage{lmodern}  % For \texttt font style
\usepackage{booktabs}  % For table formatting
\usepackage{multirow}  % Required for \multirow in tables
\usepackage{emoji}  % Required for Hugging Face emoji
\usepackage[numbers, sort&compress]{natbib}  % For citations and references
\bibliographystyle{unsrtnat}
\usepackage[utf8]{inputenc}

% Define macros, including color codes and macros
\definecolor{YonseiBlue}{HTML}{183772}

% TODO: Replace "CAS2105 Homework 6: Mini AI Pipeline Project \emoji{hugging-face}" with your report title
\title{CAS2105 Homework 6: Mini AI Pipeline Project \\\textbf{Sentiment Classification of Movie Reviews using Naive Bayes} \emoji{hugging-face}}  
  % Title


% TODO: Replace "Your Name (Your Student ID)" with your name and student ID
\author{NUR BAISHAH BINTI MUHAMMAD FAKHRI (2024148049)}


% Redefine title style
\makeatletter
  \def\@maketitle{%
  \newpage
  \null
  \vskip 2em%
  \begin{flushleft}%
  {\color{YonseiBlue}\rule{\textwidth}{2pt}}
  \vskip 5pt%
  \let \footnote \thanks
    {\Large \bf \@title \par}%
    \vskip 1.5em%
    {\normalsize \bf \lineskip .5em% 
        \@author
        \vskip 2pt%
        \par}
    {\color{YonseiBlue}\rule{\textwidth}{2pt}}
    \vskip 1.5em
  \end{flushleft}%
  }
\makeatother



\begin{document}
\maketitle


% TODO: Replace the following sections with your own report!


\section{Introduction}
\label{sec:introduction}

% --- TODO: Students fill this in ---
% Describe your chosen task and why it is interesting.
% Keep the explanation clear enough for a classmate to understand.

This project provides a gentle introduction to designing simple \textbf{AI pipelines}. Rather than training large models or reading extensive research literature, you will:

\begin{itemize}
    \item Choose a small, concrete problem solvable with an AI pipeline (e.g., text classification, retrieval, simple QA, image classification).
    \item Choose or collect a small dataset (e.g., from \texttt{datasets}).
    \item Implement a simple \emph{na\"ive baseline} (e.g., rule-based or heuristic).
    \item Build an improved pipeline using existing pre-trained models.
    \item Evaluate both approaches using appropriate metrics.
    \item Reflect on what worked, what failed, and what you learned.
\end{itemize}

The emphasis is on the \emph{process} of AI work: problem definition, pipeline design, evaluation, iteration, and writing. Your problem should be small enough to run comfortably on a single GPU (e.g., RTX~3090) or CPU.

\paragraph{Your tasks.} 
Please replace all the sections to complete your report, starting from adding your project title, your name, and student ID above! Feel free to reorganize the sections. Then, submit a \textbf{public} github repository link that includes your code (including Jupyter notebook with results) and report \textbf{in PDF}, via LearnUs. 

You can use tables and figures, and cite references using \verb|\citep| (\citep{wei2022finetuned}) or \verb|\citet| (\citet{wei2022finetuned}).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Task Definition}
\label{sec:definition}
\begin{itemize}
    \item \textbf{Task description:} In this project, I try to classify movie reviews into two categories: \textit{positive} and \textit{negative}. The goal is to train a simple model that can look at a piece of text (a review) and guess the sentiment.
    \item \textbf{Motivation:} I think this task interesting as sentiment classification is one of the most common and practical tasks in NLP. It is used in many real applications such as analyzing customer feedback, filtering reviews, or summarizing public opinion. Even though it sounds simple, getting a model to understand feelings from text is not always easy, so I wanted to explore how well a small model can perform.\\
    \item \textbf{Input / Output:} The input is a raw movie review (a string of text).  
    The output is a label: either \texttt{pos} (positive) or \texttt{neg} (negative).\\
    \item \textbf{Success criteria:} I consider the system “good” if it achieves reasonable accuracy (around 65–75\%) on unseen data and if the predictions look sensible for new reviews I type in myself.
\end{itemize}

\section{Methods}
\label{sec:methods}

This section includes both the na\"ive baseline and the improved AI pipeline.

\subsection{Na\"ive Baseline}
\label{subsec:baseline}

Implement a simple method that does not rely on heavy models. Examples include:
\begin{itemize}
    \item Keyword-based text classification,
    \item Simple color/shape heuristics for image tasks,
    \item String-overlap–based retrieval.
\end{itemize}

In your report, explain:
\begin{itemize}
    \item How the baseline works,
    \item Why it is considered na\"ive,
    \item Expected failure cases.
\end{itemize}

\subsubsection*{Your Baseline (Keyword-Based Sentiment Classifier)}
\begin{itemize}
    \item \textbf{Method description:} For my naive baseline, I designed a simple keyword-based classifier. The idea is straightforward: I created a small list of “positive” words (such as \textit{good, amazing, love}) and a small list of “negative” words (such as \textit{bad, terrible, boring}). For any review, I counted how many positive words and negative words appear. If the review contains more positive words, I classify it as positive; otherwise as negative.
    \item \textbf{Why na\"ive:} This method does not understand grammar or meaning. It cannot handle negation like “not good” and fails when the review uses words outside my small keyword list.

    \item \textbf{Likely failure modes:} The baseline will fail if:
    \begin{itemize}
        \item the review uses synonyms not in my keyword list,
        \item the sentiment is implied indirectly,
        \item the sentence contains negation (e.g., “not amazing”),
        \item the review is long and mixed with both positive and negative words.
    \end{itemize}
\end{itemize}


\subsection{AI Pipeline}
\label{subsec:pipeline}

Design a small pipeline using one or more pre-trained models. Examples include:

\begin{itemize}
    \item \textbf{Text:} embedding encoder + classifier, or a small transformer model,
    \item \textbf{Retrieval:} embedding model + nearest-neighbor search,
    \item \textbf{Vision:} pre-trained classifier (e.g., ViT-tiny).
\end{itemize}

A typical pipeline contains:
\begin{enumerate}
    \item Preprocessing,
    \item Embedding or representation,
    \item Decision/ranking component,
    \item Optional post-processing.
\end{enumerate}

Fine-tuning large models is not required; inference-only usage is sufficient.

\subsubsection*{Your Pipeline \\(Bag-of-Words + Naive Bayes Sentiment Classifier)}
\begin{itemize}
    \item \textbf{Models used:}  I used the \texttt{Multinomial Naive Bayes} classifier from scikit-learn, combined with \texttt{CountVectorizer} to turn text into numerical features. The pipeline does not rely on large transformers, so it is lightweight and easy to run.
    \item \textbf{Pipeline stages:} \begin{enumerate}
        \item \textbf{Preprocessing:}  
        I used NLTK to load movie reviews and scikit-learn's \texttt{CountVectorizer} to convert text into bag-of-words features. I also removed English stopwords.
        
        \item \textbf{Embedding / Representation:}  
        I used a bag-of-words representation with unigrams and bigrams (\texttt{ngram\_range=(1,2)}), and limited the vocabulary to 3000 features to avoid overfitting on such a small dataset.
        
        \item \textbf{Decision component:}  
        I trained a Multinomial Naive Bayes classifier on the vectorized text.
        
        \item \textbf{Post-processing:}  
        The predicted class (\texttt{pos}/\texttt{neg}) is returned directly without additional processing.
    \end{enumerate}
    \item \textbf{Design choices and justification:} I chose Naive Bayes because it is simple, fast, and known to perform surprisingly well on text classification. Using bigrams helps capture short phrases like “not good”. Limiting to 300 samples and makes the experiment lightweight. The overall pipeline is easy to understand, explain, and reproduce.
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}
\label{sec:experiments}


\subsection{Datasets}
\label{sec:experiments:datasets}

% --- TODO: Students fill this in ---
% Describe your dataset clearly.

You may use a small public dataset (e.g., from \texttt{datasets}) or construct your own. 
In this section, describe:

\begin{itemize}
    \item \textbf{Dataset source}: where it comes from.
    \item \textbf{Size}: number of examples used.
    \item \textbf{Splits}: how you divided train/validation/test.
    \item \textbf{Preprocessing}: e.g., tokenization, resizing, truncation, normalization.
\end{itemize}

\subsection*{Your Dataset Description}
\begin{itemize}
    \item \textbf{Source:} I used the built-in \texttt{movie\_reviews} dataset from NLTK. It contains human-written reviews labeled as positive or negative.
    \item \textbf{Total examples:} Although the full dataset has 2000 reviews, I limited my sample to exactly \textbf{300 reviews}.
    \item \textbf{Train/Test split:} I used an 80/20 split:  
    240 reviews for training,  
    60 reviews for testing.
    \item \textbf{Preprocessing steps:} \begin{itemize}
        \item converted raw text into bag-of-words vectors,
        \item removed English stopwords,
        \item included unigrams and bigrams,
        \item limited vocabulary to 3000 words,
        \item randomized the dataset before sampling.
    \end{itemize}
\end{itemize}


\subsection{Metrics}
\label{sec:experiments:metrics}

Use at least one quantitative metric appropriate for your task:
\begin{itemize}
    \item \textbf{Classification:} accuracy, precision, recall, F1,
    \item \textbf{Retrieval:} precision@k, recall@k,
    \item \textbf{Simple generation:} exact match, ROUGE-1.
\end{itemize}

\textcolor{gray}{It’s worth considering how the metrics you select align with your tasks.}
For evaluating my sentiment classifier, I decided to use four metrics: \textbf{accuracy}, \textbf{precision}, \textbf{recall}, and \textbf{F1-score}. These are very common in text classification, and they fit my task well.

I didn’t want to rely on accuracy alone because sometimes accuracy can look “good” even if the model is actually bad at detecting one of the classes. Since my dataset is small and not perfectly balanced, I needed metrics that show the detailed behavior of the model.

\begin{itemize}
    \item \textbf{Accuracy} tells me the overall percentage of correct predictions.
    \item \textbf{Precision} shows how many of the reviews the model predicted as positive/negative were actually correct.
    \item \textbf{Recall} tells me how many true positive or true negative reviews the model successfully found.
    \item \textbf{F1-score} is helpful because it combines both precision and recall into a single number, especially when one class is harder to classify than the other.
\end{itemize}

I chose these metrics because they give a clearer picture of what my model is doing right and where it is struggling. In sentiment analysis, it’s important not only to predict correctly but also to avoid consistently missing or mislabeling emotional expressions. Using these four metrics helps me compare the baseline and my AI pipeline more fairly.

\subsection{Results}
\label{sec:experiments:results}

Report:
\begin{itemize}
    \item Metric values for baseline vs.\ pipeline,
    \item A results table,
    \item At least three qualitative examples.
\end{itemize}


In this section, I report the performance of both the naive baseline and the Naive Bayes pipeline. I include quantitative results (metrics, tables, graphs) and qualitative examples (sample predictions).

\subsection{Baseline Model Performance}

The keyword-based baseline achieved:

\[
\textbf{Accuracy = 0.5333}
\]

\subsubsection*{Classification Report}
\begin{verbatim}
              precision    recall  f1-score   support
    neg          0.78       0.21      0.33        33
    pos          0.49       0.93      0.64        27
accuracy                            0.53        60
\end{verbatim}

\subsubsection*{Confusion Matrix}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.4\textwidth]{imgs/keyword.png}
    \caption{Confusion matrix for the baseline keyword-based classifier. It shows strong bias toward predicting positive sentiment, mainly due to limited and simplistic keyword rules.}
\end{figure}

\subsection{Naive Bayes Pipeline Performance}

The Naive Bayes model achieved:

\[
\textbf{Accuracy = 0.7167}
\]

\subsubsection*{Classification Report}
\begin{verbatim}
              precision    recall  f1-score   support
    neg          0.79       0.67      0.72        33
    pos          0.66       0.78      0.71        27
accuracy                            0.72        60
\end{verbatim}

\subsubsection*{Confusion Matrix}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.4\textwidth]{imgs/bayes.png}
    \caption{Confusion matrix for the Naive Bayes classifier. The model provides better balance across classes and corrects many of the baseline’s previous errors.}
\end{figure}
\newpage
\subsection{Metric Comparison}

To compare both models clearly, I plotted accuracy, precision, recall, and F1-score side by side.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{imgs/comparison.png}
    \caption{Performance comparison between the baseline and the Naive Bayes pipeline across four metrics: accuracy, precision, recall, and F1-score. The pipeline consistently outperforms the baseline in every metric.}
\end{figure}
\newpage
\subsection{Qualitative Examples}

I evaluated three sample movie reviews using both models.

\subsubsection*{Example 1}
\textit{“The movie started slow and felt boring at first, but overall it turned out to be surprisingly good and enjoyable.”}

\begin{itemize}
    \item Baseline prediction: \texttt{pos}
    \item Pipeline prediction: \texttt{pos}
\end{itemize}

\subsubsection*{Example 2}
\textit{“This film was absolutely terrible. The acting was awful and I hated every minute.”}

\begin{itemize}
    \item Baseline prediction: \texttt{neg}
    \item Pipeline prediction: \texttt{neg}
\end{itemize}

\subsubsection*{Example 3}
\textit{“It's not the worst movie ever made, but I wouldn't call it good either. Some parts worked, others didn't.”}

\begin{itemize}
    \item Baseline prediction: \texttt{pos}
    \item Pipeline prediction: \texttt{neg}
\end{itemize}

These qualitative results show that the baseline is easily influenced by single positive words, while the pipeline better captures context and mixed sentiment.


\section{Discussion}
The results demonstrate that the Naive Bayes pipeline significantly outperforms the keyword-based baseline. While the baseline achieves reasonable precision for negative reviews, it struggles heavily with recall. The Naive Bayes model performs consistently across all metrics due to its ability to:
\begin{itemize}
    \item Learn from word distributions,
    \item Consider the entire vocabulary rather than a small keyword list,
    \item Capture more nuanced sentiment patterns.
\end{itemize}

\section{Conclusion}
The Naive Bayes model achieved more than a 17\% accuracy improvement over the baseline. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Reflection and Limitations}
\label{sec:reflection}

Write approximately 6--10 sentences reflecting on:
\begin{itemize}
    \item What worked better than expected,
    \item What failed or was difficult,
    \item How well your metric captured ``quality'',
    \item What you would try next with more time or compute.
\end{itemize}

\subsection*{Your Reflection }
% Students write freely here.
Working on this project, I was surprised that such a simple Naive Bayes model could reach around 70\% accuracy using only 300 samples. The bag-of-words approach worked better than I expected, especially after adding bigrams. However, I also saw many limitations. The model does not really “understand” the meaning of the sentences and sometimes misclassifies short or ambiguous reviews. Another challenge is that the metric (accuracy) does not fully capture when the model is confident or confused. With more time, I would try using a small transformer model or add more preprocessing steps such as lemmatization. I would also experiment with different vectorizers (TF–IDF) or hyperparameters. Overall, this project helped me understand both the strengths and weaknesses of classical NLP methods.



\bibliography{references}
\appendix
\section{Dataset Source}

The dataset used in this project comes from the NLTK \texttt{movie\_reviews} corpus.  
It is publicly available and commonly used for sentiment analysis research and teaching.

\noindent Link: \\
\url{https://www.nltk.org/nltk_data/}


\end{document}
